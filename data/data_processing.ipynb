{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os \n",
    "import sys\n",
    "from Bio import SeqIO\n",
    "from esm import pretrained\n",
    "from esm import data\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class StringDB_Dataset(Dataset):\n",
    "    # esm_model must end in .pt for local model.\n",
    "    def __init__(self,data_path,fasta_path,esm_model='ESM3_OPEN_SMALL'):\n",
    "        self.data_path = data_path\n",
    "        self.fasta_path = fasta_path\n",
    "        self.esm_model = esm_model\n",
    "        self.data_cols = self.get_column_names()\n",
    "        '''\n",
    "        self.data = self.data.dropna()\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.data = self.data.drop_duplicates()\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.data = self.data.drop(columns=['Unnamed: 0'])\n",
    "        '''\n",
    "    \n",
    "    def get_column_names(self):\n",
    "        if os.path.exists(self.data_path):\n",
    "            with open(self.data_path) as f:\n",
    "                return f.readline().strip().split('\\t')\n",
    "        else:\n",
    "            Exception('File not found, check filepath')\n",
    "    '''\n",
    "    def get_esm_embeddings(self):\n",
    "        # Adapted from https://github.com/facebookresearch/esm/blob/main/scripts/extract.py\n",
    "        model, alphabet = pretrained.load_model_and_alphabet(self.esm_model,device='cuda')\n",
    "        model.eval()\n",
    "        dataset = data.FastaBatchedDataset.from_file(self.fasta_path)\n",
    "        batches = dataset.get_batch_indices(args.toks_per_batch, extra_toks_per_seq=1)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, collate_fn=alphabet.get_batch_converter(args.truncation_seq_length), batch_sampler=batches)\n",
    "        print(f\"Read {args.fasta_file} with {len(dataset)} sequences\")\n",
    "\n",
    "        args.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return_contacts = \"contacts\" in args.include\n",
    "\n",
    "        assert all(-(model.num_layers + 1) <= i <= model.num_layers for i in args.repr_layers)\n",
    "        repr_layers = [(i + model.num_layers + 1) % (model.num_layers + 1) for i in args.repr_layers]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "                print(\n",
    "                    f\"Processing {batch_idx + 1} of {len(batches)} batches ({toks.size(0)} sequences)\"\n",
    "                )\n",
    "                if torch.cuda.is_available() and not args.nogpu:\n",
    "                    toks = toks.to(device=\"cuda\", non_blocking=True)\n",
    "\n",
    "                out = model(toks, repr_layers=repr_layers, return_contacts=return_contacts)\n",
    "\n",
    "                logits = out[\"logits\"].to(device=\"cpu\")\n",
    "                representations = {\n",
    "                    layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()\n",
    "                }\n",
    "                if return_contacts:\n",
    "                    contacts = out[\"contacts\"].to(device=\"cpu\")\n",
    "\n",
    "                for i, label in enumerate(labels):\n",
    "                    args.output_file = args.output_dir / f\"{label}.pt\"\n",
    "                    args.output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    result = {\"label\": label}\n",
    "                    truncate_len = min(args.truncation_seq_length, len(strs[i]))\n",
    "                    # Call clone on tensors to ensure tensors are not views into a larger representation\n",
    "                    # See https://github.com/pytorch/pytorch/issues/1995\n",
    "                    if \"per_tok\" in args.include:\n",
    "                        result[\"representations\"] = {\n",
    "                            layer: t[i, 1 : truncate_len + 1].clone()\n",
    "                            for layer, t in representations.items()\n",
    "                        }\n",
    "                    if \"mean\" in args.include:\n",
    "                        result[\"mean_representations\"] = {\n",
    "                            layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                            for layer, t in representations.items()\n",
    "                        }\n",
    "                    if \"bos\" in args.include:\n",
    "                        result[\"bos_representations\"] = {\n",
    "                            layer: t[i, 0].clone() for layer, t in representations.items()\n",
    "                        }\n",
    "                    if return_contacts:\n",
    "                        result[\"contacts\"] = contacts[i, : truncate_len, : truncate_len].clone()\n",
    "\n",
    "                    torch.save(\n",
    "                        result,\n",
    "                        args.output_file,\n",
    "                    )\n",
    "    '''\n",
    "    \n",
    "    def load_data(self,data_path):\n",
    "        if os.path.exists(data_path):\n",
    "            data = pd.read_csv(data_path)\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chemical',\n",
       " 'protein',\n",
       " 'experimental_direct',\n",
       " 'experimental_transferred',\n",
       " 'prediction_direct',\n",
       " 'prediction_transferred',\n",
       " 'database_direct',\n",
       " 'database_transferred',\n",
       " 'textmining_direct',\n",
       " 'textmining_transferred',\n",
       " 'combined_score']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/mnt/d/data/stitchdb.txt'\n",
    "local_model = '/mnt/d/data/esm3_sm_open_v1.pt'\n",
    "data = StringDB_Dataset(path,local_model)\n",
    "data.data_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chemical\tprotein\texperimental_direct\texperimental_transferred\tprediction_direct\tprediction_transferred\tdatabase_direct\tdatabase_transferred\ttextmining_direct\ttextmining_transferred\tcombined_score\n",
      "CIDm91758680\t190486.XAC0787\t0\t0\t0\t0\t0\t0\t0\t161\t161\n",
      "CIDm91758680\t190486.XAC0788\t0\t0\t0\t0\t0\t0\t0\t187\t187\n",
      "CIDm91758680\t190486.XAC1728\t0\t0\t0\t0\t0\t0\t0\t161\t161\n",
      "CIDm91758680\t190486.XAC1855\t0\t0\t0\t0\t0\t0\t0\t210\t210\n",
      "CIDm91758680\t190486.XAC2361\t0\t0\t0\t0\t0\t0\t0\t161\t161\n",
      "CIDm91758680\t190486.XAC2462\t0\t0\t0\t0\t0\t0\t0\t173\t173\n",
      "CIDm91758680\t190486.XAC2928\t0\t0\t0\t0\t0\t0\t0\t161\t161\n",
      "CIDm91758680\t190486.XAC3041\t0\t0\t0\t0\t0\t0\t0\t161\t161\n",
      "CIDm91758680\t190486.XAC3368\t0\t0\t0\t0\t0\t0\t0\t161\t161\n"
     ]
    }
   ],
   "source": [
    "with open(path, 'r') as file:\n",
    "    for _ in range(10):\n",
    "        print(file.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein1 protein2 homology experiments experiments_transferred database database_transferred textmining textmining_transferred combined_score\n",
      "23.BEL05_00025 23.BEL05_06890 0 0 738 0 194 0 0 779\n",
      "23.BEL05_00025 23.BEL05_19855 0 0 264 0 0 0 0 264\n",
      "23.BEL05_00025 23.BEL05_17340 0 0 0 0 134 0 64 154\n",
      "23.BEL05_00025 23.BEL05_06420 0 0 597 0 194 0 66 670\n",
      "23.BEL05_00030 23.BEL05_09555 0 0 208 0 0 0 0 208\n",
      "23.BEL05_00030 23.BEL05_04075 0 0 317 0 0 0 0 317\n",
      "23.BEL05_00030 23.BEL05_05440 0 0 270 0 0 0 0 270\n",
      "23.BEL05_00035 23.BEL05_00165 0 0 49 0 229 0 0 235\n",
      "23.BEL05_00035 23.BEL05_09055 0 0 47 0 339 0 0 343\n"
     ]
    }
   ],
   "source": [
    "path = '/mnt/d/data/stringdb.txt'\n",
    "with open(path, 'r') as file:\n",
    "    for _ in range(10):\n",
    "        print(file.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224w",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
